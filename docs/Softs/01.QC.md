# 数据的产出

## 去除接头


### trim_galore

```bash
# trim_galore 
\_ sh -c trim_galore -q 25 --phred33 --length 25 --stringency 3 --paired -o 02.cleandata/ rawdata_all/Young-H3ac.R1.fastq.gz rawdata_all/Young-H3ac.R2.fastq.gz &>02.cleandata/Young-H3ac.log
    \_ perl /xxx/trim_galore -q 25 --phred33 --length 25 --stringency 3 --paired -o 02.cleandata/ rawdata_all/Young-H3ac.R1.fastq.gz rawdata_all/Young-H3ac.R2.fastq.gz
        \_ sh -c gzip -c - > 02.cleandata/Young-H3ac.R1_trimmed.fq.gz
        |   \_ gzip -c -
        \_ /xxx/python /xxx/cutadapt -j 1 -e 0.1 -q 25 -O 3 -a AGATCGGAAGAGC rawdata_all/Young-H3ac.R1.fastq.gz
            \_ pigz -cd rawdata_all/Young-H3ac.R1.fastq.gzip

# fq=`realpath ./cleandata/SRR11819476_trimmed.fq` 

#  encode for ATAC
cutadapt -e 0.1 -m 5 -a AGATCGGAAGAGC -A TGGAATTCTCGG treat.ATAC_R1.fq.gz treat.ATAC_R2.fq.gz -o treat.ATAC_R1.trim.fastq.gz -p treat.ATAC_R2.trim.fastq.gz
```

多次 trim 对结果影响大吗？

```bash
# 使用 trim_galore 后，又使用 encode for ATAC 对结果影响不是那么大。
# === Summary ===
# Total read pairs processed:         35,456,997
#   Read 1 with adapter:               1,026,717 (2.9%)
#   Read 2 with adapter:                 927,508 (2.6%)
# Pairs that were too short:                 328 (0.0%)
# Pairs written (passing filters):    35,456,669 (100.0%)

# 使用 trim_galore 后，使用 cutadapt
cutadapt -j 1 -e 0.1 -q 25 -O 3 -a AGATCGGAAGAGC treat.ATAC_R1_val_1.fq.gz treat.ATAC_R2_val_2.fq.gz  -o treat.ATAC_R1.trim-2.fq.gz -p treat.ATAC_R2.trim-2.fq.gz
# === Summary ===
# Total read pairs processed:         35,456,997
#   Read 1 with adapter:               1,038,836 (2.9%)
#   Read 2 with adapter:                       0 (0.0%)
# Pairs written (passing filters):    35,456,997 (100.0%)
# 使用 trim_galore 后，使用 cutadapt，再使用 cutadapt
cutadapt -j 1 -e 0.1 -q 25 -O 3 -a AGATCGGAAGAGC treat.ATAC_R1.trim-2.fq.gz treat.ATAC_R2.trim-2.fq.gz  -o treat.ATAC_R1.trim-3.fq.gz -p treat.ATAC_R2.trim-3.fq.gz
# === Summary ===
# Total read pairs processed:         35,456,997
#   Read 1 with adapter:                  34,612 (0.1%)
#   Read 2 with adapter:                       0 (0.0%)
# Pairs written (passing filters):    35,456,997 (100.0%)
cutadapt -j 1 -e 0.1 -q 25 -O 3 -a AGATCGGAAGAGC treat.ATAC_R1.trim-3.fq.gz treat.ATAC_R2.trim-3.fq.gz  -o treat.ATAC_R1.trim-4.fq.gz -p treat.ATAC_R2.trim-4.fq.gz

```


### cutadapter

```bash

```
参数理解：
- -e
  - ref：[cutadapter 软件使用之二_有大聚跟月月的蒋_新浪博客](http://blog.sina.com.cn/s/blog_c4e3e0620102x7x4.html)

【示例： -j是什么意思】
```bash
cutadapt -j 4 -a GATCGGAAGAGCACACGTCTGAACTCC --quality-base 33 -m 10 -O 4 --discard-untrimmed wy-KO-18d-3.R1.clean.fastq.gz> wy-KO-18d-3.R1.clean.trim.fastq
# -m 10 意思是舍弃掉低于10个碱基的reads
# --discard-untrimmed 意思是舍弃掉没有adapter的reads
```

---
- [ ] [cutadapter_百度搜索](https://www.baidu.com/s?wd=cutadapter&ie=UTF-8)


### fastp

[OpenGene/fastp: An ultra-fast all-in-one FASTQ preprocessor (QC/adapters/trimming/filtering/splitting/merging...)](https://github.com/OpenGene/fastp)


## Reads Description

假如比对率只有50% 左右，同时Fastqc 出现了双峰，可能怀疑有污染。


# bam 文件 描述

文库复杂度对应的英文如下

Library Complexity

表示的是文库中unique的分子数目，unique分子数目越多，文库复杂度越高。在数据分析中，重复序列会对下游分析造成影响，在snp calling, peak caling等分析前都需要去除文库中的重复序列。

只有一个复杂度高的文库，才能确保挖掘出更多有效的信息，所以在数据分析中，需要对文库的复杂度进行评估。

[使用picard评估文库复杂度_庐州月光的博客-CSDN博客](https://blog.csdn.net/weixin_43569478/article/details/108079898)
